{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-21 15:12:39--  http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30286 (30K) [application/x-httpd-php]\n",
      "Saving to: ‘auto-mpg.data.8’\n",
      "\n",
      "auto-mpg.data.8     100%[===================>]  29,58K   103KB/s    in 0,3s    \n",
      "\n",
      "2021-12-21 15:12:39 (103 KB/s) - ‘auto-mpg.data.8’ saved [30286/30286]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.data', names=cols, na_values = \"?\",\n",
    "                comment = '\\t',\n",
    "                sep= \" \",\n",
    "                skipinitialspace=True)\n",
    "\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (318, 8) \n",
      "\n",
      "Test set: (80, 8) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(data, data[\"Cylinders\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "    \n",
    "    \n",
    "print('Train set:', strat_train_set.shape, '\\n')\n",
    "print('Test set:', strat_test_set.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (318, 7) \n",
      "\n",
      "Test set: (318,) \n",
      "\n",
      "[ 9.  10.  11.  12.  13.  14.  14.5 15.  15.5 16.  16.2 16.5 16.9 17.\n",
      " 17.5 17.6 17.7 18.  18.1 18.2 18.5 18.6 19.  19.1 19.2 19.4 19.9 20.\n",
      " 20.2 20.3 20.5 20.6 20.8 21.  21.1 21.5 21.6 22.  22.3 22.4 22.5 23.\n",
      " 23.2 23.6 23.7 23.8 23.9 24.  24.2 24.3 24.5 25.  25.4 25.5 26.  26.4\n",
      " 26.5 26.6 26.8 27.  27.2 27.4 27.5 27.9 28.  28.1 28.4 28.8 29.  29.5\n",
      " 29.8 29.9 30.  30.5 30.7 30.9 31.  31.3 31.8 31.9 32.  32.1 32.2 32.3\n",
      " 32.4 33.  33.5 33.7 33.8 34.  34.1 34.2 34.3 34.4 34.5 34.7 35.  35.1\n",
      " 36.  36.1 36.4 37.  37.2 37.3 37.7 38.  39.1 39.4 40.8 43.1 43.4 44.3\n",
      " 44.6 46.6]\n"
     ]
    }
   ],
   "source": [
    "# Divide train set in train data and train labels\n",
    "train_data = strat_train_set.drop(\"MPG\", axis=1)\n",
    "train_data_labels = strat_train_set[\"MPG\"].copy()\n",
    "\n",
    "print('Train set:', train_data.shape, '\\n')\n",
    "print('Test set:', train_data_labels.shape, '\\n')\n",
    "print(np.unique(train_data_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "# Preprocess the Origin column in data\n",
    "def preprocess_origin_cols(df):\n",
    "    df[\"Origin\"] = df[\"Origin\"].map({1: \"India\", 2: \"USA\", 3: \"Germany\"})\n",
    "    return df\n",
    "\n",
    "\n",
    "acc_ix, hpower_ix, cyl_ix = 4,2, 0\n",
    "class CustomAttrAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, acc_on_power=True): # no *args or **kargs\n",
    "        self.acc_on_power = acc_on_power\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        acc_on_cyl = X[:, acc_ix] / X[:, cyl_ix]\n",
    "        if self.acc_on_power:\n",
    "            acc_on_power = X[:, acc_ix] / X[:, hpower_ix]\n",
    "            return np.c_[X, acc_on_power, acc_on_cyl]\n",
    "        \n",
    "        return np.c_[X, acc_on_cyl]\n",
    "\n",
    "def num_pipeline_transformer(data):\n",
    "    '''\n",
    "    Function to process numerical transformations\n",
    "    Argument:\n",
    "        data: original dataframe \n",
    "    Returns:\n",
    "        num_attrs: numerical dataframe\n",
    "        num_pipeline: numerical pipeline object\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Keep only numerical data\n",
    "    numerics = ['float64', 'int64']\n",
    "    num_attrs = data.select_dtypes(include=numerics)\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attrs_adder', CustomAttrAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "    return num_attrs, num_pipeline\n",
    "\n",
    "\n",
    "def pipeline_transformer(data):\n",
    "    '''\n",
    "    Complete transformation pipeline for numerical and categorical data.    \n",
    "    Argument:\n",
    "        data: original dataframe \n",
    "    Returns:\n",
    "        prepared_data: preprocessed data\n",
    "    '''\n",
    "    \n",
    "    cat_attrs = [\"Origin\"]\n",
    "    \n",
    "    num_attrs, num_pipeline = num_pipeline_transformer(data)\n",
    "    \n",
    "    full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, list(num_attrs)),\n",
    "        (\"cat\", OneHotEncoder(), cat_attrs),\n",
    "        ])\n",
    "    \n",
    "    prepared_data = full_pipeline.fit_transform(data)\n",
    "    return prepared_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 11)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess train set\n",
    "preprocessed_df = preprocess_origin_cols(train_data)\n",
    "pre_train_data = pipeline_transformer(preprocessed_df)\n",
    "print(pre_train_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross validation scores: 3.0757081793709324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Linear Regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(pre_train_data, train_data_labels)\n",
    "\n",
    "lin_reg_scores = cross_val_score(lin_reg, \n",
    "                                  pre_train_data,\n",
    "                                  train_data_labels, \n",
    "                                  scoring=\"neg_mean_squared_error\", \n",
    "                                  cv = 10)\n",
    "\n",
    "lin_reg_rmse_scores = np.sqrt(-lin_reg_scores)\n",
    "print('Mean of cross validation scores:', lin_reg_rmse_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross validation scores: 3.3171661482009314\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(pre_train_data, train_data_labels)\n",
    "\n",
    "tree_reg_cv_scores = cross_val_score(tree_reg, \n",
    "                         pre_train_data, \n",
    "                         train_data_labels, \n",
    "                         scoring=\"neg_mean_squared_error\", \n",
    "                         cv = 10)\n",
    "\n",
    "tree_reg_rmse_scores = np.sqrt(-tree_reg_cv_scores)\n",
    "print('Mean of cross validation scores:', tree_reg_rmse_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aspa/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross validation scores: 2.587017664726406\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(pre_train_data, train_data_labels)\n",
    "\n",
    "forest_reg_cv_scores = cross_val_score(forest_reg,\n",
    "                                         pre_train_data,\n",
    "                                         train_data_labels,\n",
    "                                         scoring='neg_mean_squared_error',\n",
    "                                         cv = 10)\n",
    "\n",
    "forest_reg_rmse_scores = np.sqrt(-forest_reg_cv_scores)\n",
    "print('Mean of cross validation scores:', forest_reg_rmse_scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of cross validation scores: 3.08659162080283\n"
     ]
    }
   ],
   "source": [
    "# SVM Regressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel='linear')\n",
    "svm_reg.fit(pre_train_data, train_data_labels)\n",
    "\n",
    "svm_cv_scores = cross_val_score(svm_reg, \n",
    "                                pre_train_data,\n",
    "                                train_data_labels,\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                cv = 10)\n",
    "\n",
    "svm_rmse_scores = np.sqrt(-svm_cv_scores)\n",
    "print('Mean of cross validation scores:', svm_rmse_scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest gives the best results (smallest error) using the cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by the Grid search method:\n",
      " {'max_features': 8, 'n_estimators': 30}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aspa/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Grid search for hyperparameter tuning \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Choose the hyperparameters thath you want to tune\n",
    "param_grid = [\n",
    "    # explore 2 grids of parameters\n",
    "    {'n_estimators': [3, 10, 30], \n",
    "     'max_features': [2, 4, 6, 8]},\n",
    "    \n",
    "    {'bootstrap': [False], \n",
    "     'n_estimators': [3, 10],\n",
    "     'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "# Choose model for hyperparameter tuning\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True,\n",
    "                           cv=10,\n",
    "                          )\n",
    "\n",
    "grid_search.fit(pre_train_data, train_data_labels)\n",
    "print('Best parameters found by the Grid search method:\\n', grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 2, 'n_estimators': 3}  --> Neg_mean_squared_error:  3.2764517802561683\n",
      "{'max_features': 2, 'n_estimators': 10}  --> Neg_mean_squared_error:  3.006469962467632\n",
      "{'max_features': 2, 'n_estimators': 30}  --> Neg_mean_squared_error:  2.9518091137512865\n",
      "{'max_features': 4, 'n_estimators': 3}  --> Neg_mean_squared_error:  3.437932743868426\n",
      "{'max_features': 4, 'n_estimators': 10}  --> Neg_mean_squared_error:  2.8758091560266643\n",
      "{'max_features': 4, 'n_estimators': 30}  --> Neg_mean_squared_error:  2.711621770736692\n",
      "{'max_features': 6, 'n_estimators': 3}  --> Neg_mean_squared_error:  2.9940629582397835\n",
      "{'max_features': 6, 'n_estimators': 10}  --> Neg_mean_squared_error:  2.889617447640897\n",
      "{'max_features': 6, 'n_estimators': 30}  --> Neg_mean_squared_error:  2.8172615823172262\n",
      "{'max_features': 8, 'n_estimators': 3}  --> Neg_mean_squared_error:  3.2118877503789327\n",
      "{'max_features': 8, 'n_estimators': 10}  --> Neg_mean_squared_error:  2.788361199002136\n",
      "{'max_features': 8, 'n_estimators': 30}  --> Neg_mean_squared_error:  2.689552645174847\n",
      "{'bootstrap': False, 'max_features': 2, 'n_estimators': 3}  --> Neg_mean_squared_error:  3.405087758268417\n",
      "{'bootstrap': False, 'max_features': 2, 'n_estimators': 10}  --> Neg_mean_squared_error:  2.8832679310574223\n",
      "{'bootstrap': False, 'max_features': 3, 'n_estimators': 3}  --> Neg_mean_squared_error:  3.381675658391923\n",
      "{'bootstrap': False, 'max_features': 3, 'n_estimators': 10}  --> Neg_mean_squared_error:  2.8525809785164222\n",
      "{'bootstrap': False, 'max_features': 4, 'n_estimators': 3}  --> Neg_mean_squared_error:  2.833544818842881\n",
      "{'bootstrap': False, 'max_features': 4, 'n_estimators': 10}  --> Neg_mean_squared_error:  2.8836456550954632\n"
     ]
    }
   ],
   "source": [
    "# Print all combinations with their scores\n",
    "cv_scores = grid_search.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cv_scores['mean_test_score'], cv_scores[\"params\"]):\n",
    "    print(params, ' --> Neg_mean_squared_error: ', np.sqrt(-mean_score) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acc_on_power', 0.02235251704428772),\n",
       " ('acc_on_cyl', 0.015615722344495257),\n",
       " ('Weight', 0.2010595237410475),\n",
       " ('Model Year', 0.12083560068070626),\n",
       " ('Horsepower', 0.11470534342336992),\n",
       " ('Displacement', 0.3693649248140762),\n",
       " ('Cylinders', 0.1328526478136263),\n",
       " ('Acceleration', 0.015304533342734176)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the best set of hyperparameters and compute the importance of the features\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "\n",
    "extra_attrs = [\"acc_on_power\", \"acc_on_cyl\"]\n",
    "numerics = ['float64', 'int64']\n",
    "num_attrs = list(train_data.select_dtypes(include=numerics))\n",
    "\n",
    "attrs = num_attrs + extra_attrs\n",
    "sorted(zip(attrs, feature_importances), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Results:\n",
      " MSE: 8.888482361111112 \n",
      " RMSE: 2.9813557924392575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use the best model with the optimal parameters\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Preprocess test data\n",
    "test_data = strat_test_set.drop(\"MPG\", axis=1)\n",
    "test_data_labels = strat_test_set[\"MPG\"].copy()\n",
    "\n",
    "preprocessed_test_data = preprocess_origin_cols(test_data)\n",
    "pre_test_data = pipeline_transformer(preprocessed_test_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = final_model.predict(pre_test_data)\n",
    "mse = mean_squared_error(test_data_labels, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('Prediction Results:\\n MSE:', mse, '\\n RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model\n",
    "with open(\"final_trained_model.bin\", 'wb') as f_out:\n",
    "    pickle.dump(final_model, f_out)\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted fuel consumption values:  [33.14       18.48333333 19.26      ]\n"
     ]
    }
   ],
   "source": [
    "# Check if saved model can be imported and used to make prediction for a random example\n",
    "\n",
    "\n",
    "# 3 random examples\n",
    "vehicle_config = {\n",
    "    'Cylinders': [4, 6, 8],\n",
    "    'Displacement': [155.0, 160.0, 165.5],\n",
    "    'Horsepower': [93.0, 130.0, 98.0],\n",
    "    'Weight': [2500.0, 3150.0, 2600.0],\n",
    "    'Acceleration': [15.0, 14.0, 16.0],\n",
    "    'Model Year': [81, 80, 78],\n",
    "    'Origin': [3, 2, 1]\n",
    "}\n",
    "\n",
    "\n",
    "# Load saved model\n",
    "with open('final_trained_model.bin', 'rb') as f_in:\n",
    "    model = pickle.load(f_in)\n",
    "    \n",
    "    \n",
    "# Make predictions   \n",
    "def predict_mpg(config, model):\n",
    "    \n",
    "    if type(config) == dict:\n",
    "        df = pd.DataFrame(config)\n",
    "    else:\n",
    "        df = config\n",
    "    \n",
    "    preproc_df = preprocess_origin_cols(df)\n",
    "    prepared_df = pipeline_transformer(preproc_df)\n",
    "    y_pred = model.predict(prepared_df)\n",
    "    return y_pred\n",
    "\n",
    "print('Predicted fuel consumption values: ', predict_mpg(vehicle_config, model))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
